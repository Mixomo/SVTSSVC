{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Check the GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0gQcIZ8RsOkn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installs mamba and updates base\n",
        "\n",
        "%%shell\n",
        "curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\n",
        "chmod +x Mambaforge-Linux-x86_64.sh\n",
        "bash Mambaforge-Linux-x86_64.sh -b -f -p /usr/local\n",
        "mamba config --env --set always_yes true\n",
        "rm Mambaforge-Linux-x86_64.sh\n",
        "mamba update -n base -c defaults conda -y"
      ],
      "metadata": {
        "id": "DLF3hGn_Nmkc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS0OPRkL4Pme",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Clone github repository\n",
        "!git clone https://github.com/justinjohn0306/so-vits-svc -b 4.0-v2\n",
        "%cd /content/so-vits-svc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creates mamba environment and activates it from environment.yaml. Takes ~8 minutes.\n",
        "\n",
        "%%shell\n",
        "mamba env create -f environment.yaml\n",
        "mamba init bash\n",
        "source /root/.bashrc\n",
        "source activate sovits\n",
        "pip install pyworld praat-parselmouth fairseq"
      ],
      "metadata": {
        "id": "zXBLkXxL4T1O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download necessary model files\n",
        "# 源仓库地址：[contentvec](https://github.com/auspicious3000/contentvec)\n",
        "# 模型原下载链接：[checkpoint_best_legacy_500.pt](https://ibm.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr)\n",
        "# 由于源网盘无法提供http直链，根据mit协议，对模型进行二次分发，提供下载直链\n",
        "!wget -P hubert/ https://huggingface.co/innnky/contentvec/resolve/main/checkpoint_best_legacy_500.pt"
      ],
      "metadata": {
        "id": "pCqf3W0d6ify",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preprocessing"
      ],
      "metadata": {
        "id": "k1qadJBFehMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading Google Drive\n",
        "#@markdown load Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wmUkpUmfn_Hs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This processing script can preprocess multiple speakers at one time, and generate multi-speaker filelists and corresponding configuration files\n",
        "\n",
        "Just put your dataset into the dataset_raw directory according to the following file structure\n",
        "\n",
        "\n",
        "```\n",
        "dataset_raw\n",
        "├───speaker0\n",
        "│   ├───xxx1-xxx1.wav\n",
        "│   ├───...\n",
        "│   └───Lxx-0xx8.wav\n",
        "└───speaker1\n",
        "    ├───xx2-0xxx2.wav\n",
        "    ├───...\n",
        "    └───xxx7-xxx007.wav\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kBlju6Q3lSM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the packaged dataset from Google Cloud Disk for preprocessing\n",
        "#@markdown **The data set of sovits3.0 no longer needs a specific file structure. Put all the wav files of the data set in the same folder, compress them into zip and upload them to Google Cloud Disk. This processing script can be used once When processing multiple datasets, please decompress each dataset in turn**\n",
        "\n",
        "#@markdown dataset name (**character’s English/Pinyin name**, which is the same as when creating the data folder; without zip.)\n",
        "DATASETNAME = \"kiritan\"  #@param {type:\"string\"}\n",
        "#@markdown Compressed package path (Google disk path, don’t change this if you pass it to dataset, create a new one if there is no dataset folder)\n",
        "ZIP_PATH = \"/content/drive/MyDrive/dataset/\"  #@param {type:\"string\"}\n",
        "ZIP_NAME = ZIP_PATH + DATASETNAME\n",
        "\n",
        "!unzip -d /content/so-vits-svc/dataset_raw {ZIP_NAME}.zip"
      ],
      "metadata": {
        "cellView": "form",
        "id": "U05CXlAipvJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Resample audio to 44100hz\n",
        "!python resample.py"
      ],
      "metadata": {
        "id": "_ThKTzYs5CfL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Divide training set and Generate configuration file\n",
        "!python preprocess_flist_config.py"
      ],
      "metadata": {
        "id": "svITReeL5N8K",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate hubert and f0\n",
        "!python preprocess_hubert_f0.py"
      ],
      "metadata": {
        "id": "xHUXMi836DMe",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title At this point, the preprocessing of the data set is completed, and the data set and related files are saved to the dataset folder of Google Cloud Disk, which is convenient for the next training\n",
        "#Compress the dataset folder\n",
        "!zip -r dataset.zip /content/so-vits-svc/dataset\n",
        "#@markdown Customize the name of the dataset folder under the dataset folder of Google Cloud Disk to avoid confusion\n",
        "dataset_name_drive = \"kiritan_preprocessed\"  #@param {type:\"string\"}\n",
        "DATASET_PATH_DRIVE = \"/content/drive/MyDrive/dataset/\" + dataset_name_drive\n",
        "!mkdir -p {DATASET_PATH_DRIVE}\n",
        "\n",
        "!cp /content/so-vits-svc/dataset.zip \"{DATASET_PATH_DRIVE}\"\n",
        "!cp configs/config.json \"{DATASET_PATH_DRIVE}\"\n",
        "!cp filelists/train.txt \"{DATASET_PATH_DRIVE}\"\n",
        "!cp filelists/val.txt \"{DATASET_PATH_DRIVE}\""
      ],
      "metadata": {
        "id": "Wo4OTmTAUXgj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title If the data set has been preprocessed, you can skip the preprocessing part and decompress the processed data and configuration files directly from the cloud disk\n",
        "#@markdown Load the preprocessed data set from Google Cloud Disk, the folder name is the same as what you entered when you backed up\n",
        "back_up_name = \"kiritan_preprocessed\"  #@param {type:\"string\"}\n",
        "BACK_UP_DATASET_PATH = \"/content/drive/MyDrive/dataset/\" + back_up_name\n",
        "!unzip {BACK_UP_DATASET_PATH}/dataset.zip -d /\n",
        "!cp {BACK_UP_DATASET_PATH}/config.json /content/so-vits-svc/configs/config.json \n",
        "!cp {BACK_UP_DATASET_PATH}/val.txt filelists/val.txt\n",
        "!cp {BACK_UP_DATASET_PATH}/train.txt filelists/train.txt\n",
        "\n",
        "\n",
        "# 拷贝云盘上保存的记录点\n",
        "# !cp /content/drive/MyDrive/G_800.pth logs/48k/\n",
        "# !cp /content/drive/MyDrive/D_800.pth logs/48k/"
      ],
      "metadata": {
        "id": "P2G6v_6zblWK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ENoH-pShel7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose whether to save the trained model to Google Cloud Disk and whether to use the pre-model\n",
        "\n",
        "#@markdown **Save the trained model file to Google Cloud Disk. After checking, you need to check and execute it when resuming training**\n",
        "Save_to_drive = True #@param {type:\"boolean\"}\n",
        "if Save_to_drive:\n",
        "  !rm -rf /content/so-vits-svc/logs/44k\n",
        "  !mkdir -p /content/drive/MyDrive/44k\n",
        "  !ln -s /content/drive/MyDrive/44k /content/so-vits-svc/logs/44k\n",
        "\n",
        "#@markdown **Download the pre-model for the first training and continue training after using the record points saved by yourself, no need to download again**\n",
        "\n",
        "#@markdown **Use the pre-model, check the box below to automatically download and enable**\n",
        "pre_pth = True #@param {type:\"boolean\"}\n",
        "if pre_pth:\n",
        "  !wget -P logs/44k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/sovits4.0-v2/G_0.pth\n",
        "  !wget -P logs/44k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/sovits4.0-v2/D_0.pth\n"
      ],
      "metadata": {
        "id": "l8J2ubh9KV5J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Training\n",
        "\n",
        "#@markdown **start training**\n",
        "\n",
        "#@markdown **Enable tensorboard to visualize data**\n",
        "tensorboard_on = True #@param {type:\"boolean\"}\n",
        "if tensorboard_on:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir logs/44k\n",
        "\n",
        "!python train.py -c configs/config.json -m 44k\n"
      ],
      "metadata": {
        "id": "-hEFFTCfZf57",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Manually back up the trained model files to Google Cloud Disk\n",
        "#@markdown You need to check the file name of the model under the /content/so-vits-svc/logs/44k/ folder by yourself, and manually modify the file name at the end of the command below\n",
        "!mv /content/so-vits-svc/logs/44k/G_1000.pth /content/drive/MyDrive\n",
        "!mv /content/so-vits-svc/logs/44k/D_1000.pth /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "KiNCWprSPlKH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning"
      ],
      "metadata": {
        "id": "oCnbX-OT897k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Synthetic Audio (Inference)\n",
        "#@markdown Upload the audio to the ``so-vits-svc/raw`` folder, and then set the model path, configuration file path, and synthesized audio name\n",
        "\n",
        "!python inference_main.py -m \"logs/44k/G_1600.pth\" -c \"configs/config.json\" -n \"君の知らない物語-src.wav\" -s kiritan\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dYnKuKTIj3z1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}